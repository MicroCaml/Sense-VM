{- | This module implements a tokenizer that "Preprocessor" uses to perform layout
resolution. The two most important functions are `tokenize` and `printTokPos`. Some
definitions in this module are stolen from the kind of code that is generated by the
BNFC tool. -}
{-# LANGUAGE OverloadedStrings #-}
module CamIoT.Parser.Tokenize where

import           CamIoT.Internal.Syntax
import           CamIoT.Parser.Keywords
import qualified Data.Text                     as T

import           Data.Char
import           Data.List
import           Data.Maybe

-- | A token stream is a list of `TokPos`
type TokenStream = [TokPos]

-- | Split a source file up into its tokens. I am guessing this is grossly inefficient.
tokenize :: T.Text -> [TokPos]
tokenize t = concat $ zipWith tokenizeLine [1 ..] (T.lines t)

-- | Turn a list of tokens back into a source file `Text` object.
printTokPos :: [TokPos] -> T.Text
printTokPos ts = T.unlines $ map tokline $ groupBy pred ts
  where pred (_, l1, _) (_, l2, _) = l1 == l2

-- | Data type of tokens (definition stolen from the stuff BNFC generates)
data Tok =
   TS T.Text !Int    -- ^ reserved words and symbols (not sure what the unique int is for)
 | TL T.Text         -- ^ string literals
 | TI T.Text         -- ^ integer literals
 | TV T.Text         -- ^ identifiers
 | TD T.Text         -- ^ double precision float literals
 | T_UIdent T.Text   -- ^ uppercase identifiers
 deriving Show

-- | A Token with a position
type TokPos = (Tok, Int, Int)

-- | Takes a `TokPos` and returns the `Text` hidden within it.
toktext :: TokPos -> T.Text
toktext (t, _, _) = case t of
  TS t _     -> t
  TL       t -> t
  TI       t -> t
  TV       t -> t
  TD       t -> t
  T_UIdent t -> t

{- | Turn a list of TokPos into a line of Text. Prepend the line with the indentation
level as specified by the first token. The list of tokens are assumed to all be on the
same line. Used by `printTokPos` for turning a token-stream back into a `Text` for the
parser to handle. -}
tokline :: [TokPos] -> T.Text
tokline []                   = ""
tokline (tok@(_, _, c) : ts) = T.append first $ T.unwords $ map toktext ts
  where first = T.snoc (T.append (T.replicate (c - 1) " ") (toktext tok)) ' '

-- | Compute the length of a token
toklength :: Tok -> Int
toklength t = case t of
  TS t _     -> T.length t
  TL       t -> T.length t
  TI       t -> T.length t
  TV       t -> T.length t
  TD       t -> T.length t
  T_UIdent t -> T.length t

-- | Takes a the line number and an actual line from the source file, and tokenizes it
tokenizeLine :: Int -> T.Text -> [TokPos]
tokenizeLine line row = case sptb row of
  Just (count, rest) -> go line (count + 1) rest
  Nothing            -> []
 where
  {- | Takes a line number, column number and a row and returns a list of tokens
    that was tokenized from that row. -}
  go :: Int -> Int -> T.Text -> [TokPos]
  go line col row = case nextToken row of
    Just (tok, rest) -> case sptb rest of
      Just (count, rest') ->
        (tok, line, col) : go line (col + toklength tok + count) rest'
      Nothing -> [(tok, line, col)]
    Nothing -> []

-- | Tries the tokenize strategies one by one until one succeeds
nextToken :: T.Text -> Maybe (Tok, T.Text)
nextToken t = fetchToken
  [tokuiden t, tokfloat t, tokint t, tokiden t, tokreserved t]
 where
  fetchToken :: [Maybe (Tok, T.Text)] -> Maybe (Tok, T.Text)
  fetchToken []       = Nothing
  fetchToken (x : xs) = if isJust x then x else fetchToken xs

-- * Tokenize strategies

-- | int literals
tokint :: T.Text -> Maybe (Tok, T.Text)
tokint t =
  let (token, rest) = T.span (\c -> isDigit c) t
  in  if T.null token then Nothing else Just (TI token, rest)

-- | float literals
tokfloat :: T.Text -> Maybe (Tok, T.Text)
tokfloat t = do
  (TI big, rest) <- tokint t
  if T.null rest
    then Nothing
    else do
      assertB $ T.head rest == '.'
      (TI low, rest') <- tokint $ T.tail rest
      return (TD $ T.concat [big, ".", low], rest')

-- | identifiers    
tokiden :: T.Text -> Maybe (Tok, T.Text)
tokiden t = do
  assertB $ isLetter $ T.head t
  let (token, rest) = T.span pred t
  assertB $ not (T.unpack token `elem` keywords)
  return (TV token, rest)
  where pred c = isLetter c || isDigit c || c == '\'' || c == '_'

-- | uppercase identifiers
tokuiden :: T.Text -> Maybe (Tok, T.Text)
tokuiden t = do
  assertB $ isUpper $ T.head t
  let (restuid, rest) = T.span pred t
  return (T_UIdent restuid, rest)
  where pred c = isLetter c || isUpper c || c == '\'' || c == '_'

-- * Utility functions

-- | reserved words and keywords
{- I am not entirely sure what the numbers are for, this code is in large piece borrowed
from the stuff generated by BNFC. -}
tokreserved :: T.Text -> Maybe (Tok, T.Text)
tokreserved t
  | "Bool" `T.isPrefixOf` t  = Just (TS "Bool" 18, T.drop (T.length "Bool") t)
  | "Int" `T.isPrefixOf` t   = Just (TS "Int" 21, T.drop (T.length "Int") t)
  | "Float" `T.isPrefixOf` t = Just (TS "Float" 20, T.drop (T.length "Float") t)
  | "True" `T.isPrefixOf` t  = Just (TS "True" 22, T.drop (T.length "True") t)
  | "False" `T.isPrefixOf` t = Just (TS "False" 19, T.drop (T.length "False") t)
  | "data" `T.isPrefixOf` t  = Just (TS "data" 27, T.drop (T.length "data") t)
  | "where" `T.isPrefixOf` t = Just (TS "where" 34, T.drop (T.length "where") t)
  | "case" `T.isPrefixOf` t  = Just (TS "case" 26, T.drop (T.length "case") t)
  | "of" `T.isPrefixOf` t    = Just (TS "of" 32, T.drop (T.length "of") t)
  | "let" `T.isPrefixOf` t   = Just (TS "let" 31, T.drop (T.length "let") t)
  | "in" `T.isPrefixOf` t    = Just (TS "in" 30, T.drop (T.length "in") t)
  | "if" `T.isPrefixOf` t    = Just (TS "if" 29, T.drop (T.length "if") t)
  | "then" `T.isPrefixOf` t  = Just (TS "then" 33, T.drop (T.length "then") t)
  | "else" `T.isPrefixOf` t  = Just (TS "else" 28, T.drop (T.length "else") t)
  | ":" `T.isPrefixOf` t     = Just (TS ":" 12, T.drop (T.length ":") t)
  | "->" `T.isPrefixOf` t    = Just (TS "->" 10, T.drop (T.length "->") t)
  | "{" `T.isPrefixOf` t     = Just (TS "{" 35, T.drop (T.length "{") t)
  | "}" `T.isPrefixOf` t     = Just (TS "}" 37, T.drop (T.length "}") t)
  | ";" `T.isPrefixOf` t     = Just (TS ";" 13, T.drop (T.length ";") t)
  | "()" `T.isPrefixOf` t    = Just (TS "()" 4, T.drop (T.length "()") t)
  | "(" `T.isPrefixOf` t     = Just (TS "(" 3, T.drop (T.length "(") t)
  | ")" `T.isPrefixOf` t     = Just (TS ")" 5, T.drop (T.length ")") t)
  | "+" `T.isPrefixOf` t     = Just (TS "+" 7, T.drop (T.length "+") t)
  | "-" `T.isPrefixOf` t     = Just (TS "-" 9, T.drop (T.length "-") t)
  | "*" `T.isPrefixOf` t     = Just (TS "*" 6, T.drop (T.length "*") t)
  | "/" `T.isPrefixOf` t     = Just (TS "/" 11, T.drop (T.length "/") t)
  | "&&" `T.isPrefixOf` t    = Just (TS "&&" 2, T.drop (T.length "&&") t)
  | "||" `T.isPrefixOf` t    = Just (TS "||" 36, T.drop (T.length "||") t)
  | "!" `T.isPrefixOf` t     = Just (TS "!" 1, T.drop (T.length "!") t)
  | "," `T.isPrefixOf` t     = Just (TS "," 8, T.drop (T.length ",") t)
  | "<=" `T.isPrefixOf` t    = Just (TS "<=" 38, T.drop (T.length "<=") t)
  | "<" `T.isPrefixOf` t     = Just (TS "<" 14, T.drop (T.length "<") t)
  | ">=" `T.isPrefixOf` t    = Just (TS ">=" 39, T.drop (T.length ">=") t)
  | ">" `T.isPrefixOf` t     = Just (TS ">" 17, T.drop (T.length ">") t)
  | "==" `T.isPrefixOf` t    = Just (TS "==" 16, T.drop (T.length "==") t)
  | "=" `T.isPrefixOf` t     = Just (TS "=" 15, T.drop (T.length "=") t)
  | "\\" `T.isPrefixOf` t    = Just (TS "\\" 23, T.drop (T.length "\\") t)
  | "_" `T.isPrefixOf` t     = Just (TS "_" 24, T.drop (T.length "_") t)
  | "as" `T.isPrefixOf` t    = Just (TS "as" 25, T.drop (T.length "as") t)
tokreserved _ = Nothing

{- | Consumes whitespace and tabs, and returns the number of items that were dropped,
together with the remainder of the `Text`. -}
sptb :: T.Text -> Maybe (Int, T.Text)
sptb t =
  let (chunk, rest) = T.span pred t
  in  if T.null rest then Nothing else Just (T.length chunk, rest)
  where pred c = c == ' ' || c == '\t'

-- | Return @Nothing@ if the boolean is @False@, otherwise @Just ()@
assertB :: Bool -> Maybe ()
assertB True  = Just ()
assertB False = Nothing
